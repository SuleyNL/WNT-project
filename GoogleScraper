#pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib
#pip install googlesearch-python
import time
from googlesearch import search
import mysql.connector
import requests
import bs4 as bs
import urllib
from datetime import datetime, timedelta

import scrapy
from scrapy.crawler import CrawlerProcess
from io import StringIO
from functools import partial
from scrapy.http import Request
from scrapy.spiders import Spider
from scrapy.selector import Selector
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy.item import Item
import pandas as pd
from pandas import DataFrame
import urllib3.request
import pdfkit as pdf
from jinja2 import Environment, FileSystemLoader
from scrapy.pipelines.files import FilesPipeline
import pdfkit
from selenium import webdriver


def downloadWNTList():
    # start web browser
    browser = webdriver.Chrome()

    # get source code
    browser.get("https://www.topinkomens.nl/voor-wnt-instellingen/wnt-register")
    html = browser.page_source

    # wait for page to load
    time.sleep(1)

    # get all organisations
    table = browser.find_elements_by_css_selector('tr')
    i = 0
    for line in table:
        # skip first value (title)
        if i > 0:
            name = table[i].find_elements_by_css_selector('td')[0].text.replace("\n", "")
            organisation = clean(name) + "\n"

            # write to file
            g = open("WNT-List.txt", "a")
            g.write(organisation)
            print(organisation)
            g.close()
        i = i + 1

    # close web browser
    browser.close()


def createDatabase():
    mydb = mysql.connector.connect(host="localhost",user="Suley",password="Hell0")
    print("my database: " + mydb)


def getURLs(year):

    # determine file
    outputFile = "PDFs-List/PDFs-List-%s.txt" % year
    print(outputFile)

    # open inputFile
    with open('WNT-List.txt') as inputFile:
        inputLines = inputFile.readlines()

        # check if there is already some output so it can continue from there
        with open(outputFile, "a+") as output:
            outputLines = output.readlines()
            i = 0
            lastOutputLine = 0
            if len(outputLines) > 0:
                lastOutputLine = outputLines[-1].split(":")[0]
                i = int(lastOutputLine) + 1

        # for every line in inputFile it outputs a line in outputFile
        while i < len(inputLines):
            print("___________________________________________________")
            g = open(outputFile, "a")
            foundPDFs = []

            # try except block to catch the error that occurs when we get blocked by google
            try:
                foundPDFs = searchGoogle(year, inputLines[i].replace("\n", ""))
                # proper formatting = [number]: [link1], [link2], [link3] \n
                text = str(i) + ":"
                for PDF in foundPDFs:
                    if foundPDFs.index(PDF) != 0:
                        text = text + ","
                    text = text + " " + PDF
                text = text + "\n"

                # write text to file
                g.write(text)
                g.close()
                print(text)

                # freeze program so we wont get banned by google
                # on 10 seconds it did 80
                # on 20 seconds it did 64
                #time.sleep()

                # show progress on screen
                print(str(round((i/len(inputLines))*100, 2)) + "% Completed!")
                print("Total lines this session: " + str(i-int(lastOutputLine)))
                i = i+1

            except requests.HTTPError as exception:
                print("Too many requests. Waiting 2hrs and 20min for next batch")
                nextBatchTime = datetime.now() + timedelta(hours=2.34)
                print("Next batch scheduled at: " + format(nextBatchTime, '%H:%M:%S'))
                time.sleep(60*60*2.34)




def searchGoogle(year, name):

    # prepare query you want to search, somehow didnt work if it was directly put into search() function
    template = "(Jaarverslag|Jaarrekening|WNT-verantwoording|Verslaggevingsdocument|Jaarstuk) -dnb.com -almanak.overheid.nl filetype:pdf"
    year = str(year)
    searchTerm = year + " " + name + " " + template

    # here it actually searches the web
    searchResults = search(searchTerm)

    print("searching for PDF of: " + name)
    print("in year: " + year)

    return searchResults[0:3]


def clean(name):
    # some organisations (#163) had their names double, so here that gets cleaned up.
    # the only risk with this approach is if the organisation name is symmetrical and even like 'grootoorg'.
    # in that case we probably should search both the half and the full name
    middleNumber = round(len(name) / 2)

    # determine both halves of the organisation name
    left = name[0:middleNumber]
    right = name[middleNumber:-1] + name[-1]

    # see if both halves are the same, if yes: take the left half
    if left == right:
        name = left

    return name


def selenium():
    # start web browser
    browser = webdriver.Chrome()

    # get source code
    browser.get("https://www.coc.nl/over-ons/jaarverslag")
    html = browser.page_source
    time.sleep(2)
    allLinks = browser.find_elements_by_css_selector('a')

    i=1
    for link in allLinks:
        print(str(i) + ". : " + link.text + ": " + str(link.get_attribute("href")))
        i = i+1

    #for i, element in allLinks:
    #    print(allLinks[i].get_attribute("href" + "\n"))

    # close web browser
    browser.close()

def test(year):
    outputFile = "PDFs-List/PDFs-List-%s.txt" % year
    print(outputFile)
    with open(outputFile) as output:
        outputLines = output.readlines()
        i = 0
        print(str(i))
        print(outputLines[-1])

def test():
    i = 0
    text = "Brabantse Ontwikkelings Maatschappij Holding B.V. 2018 (Jaarverslag|Jaarrekening|WNT-verantwoording|Verslaggevingsdocument|Jaarstuk) -dnb.com -almanak.overheid.nl filetype:pdf"
    i = i + 1
    #text = "https://suleymen-kandrouch-cv.web.app"
    #text = "https://stackoverflow.com/questions/38260853/bytes-object-has-no-attribute-find-all"
    time.sleep(3)

    text = urllib.parse.quote_plus(text)
    url = 'https://google.com/search?q=' + text
    response = requests.get(url).content
    page = bs.BeautifulSoup(response, "lxml")
    content = page.find_all("a::attr(href)")

    for url in page.find_all('a'):
        print(url.get('href'))

    linksList = []
    for i, item in enumerate(content):
        if 'http' in item:
            foundLink = content[i].strip("//")
            linksList.append(foundLink)
    print("Lengte van lijst: " + str(len(linksList)))
    print()
    time.sleep(0.2)

if __name__ == "__main__":
    #selenium()

    #downloadWNTList()

    #print(searchGoogle(2018, "Brabantse Ontwikkelings Maatschappij Holding B.V."))

    getURLs(2020)

    #test()